# Pipelines

`r CiteDate(20201024)` A general mlhub philosophy is that the output from
a command should be, for example, a well defined text
format. Typically this will use a **csv** (comma separated
value) format and will be consistent so that follow-on processes
within a pipeline can further process the results. These might even be
other mlhub models. The mlhub commands focus on their specific task,
not solving all problems, but implementing their specific task
well. We can then leave extra processing to other specialist tools,
like `r Command(sed)`, or `r Command(cut)`, and `r Command(awk)`.

This example deploys an optical character recognition capability from
the **ocr** command of the *azcv* model:

```{bash eval=FALSE}
$ ml ocr azcv ~/.mlhub/azcv/cache/images/mycat.png | head -2
51.0 43.0 668.0 51.0 667.0 85.0 51.0 77.0,My cats name is freckles. She like's to climb up
37.0 97.0 691.0 104.0 690.0 134.0 37.0 128.0,high. She is 2 years old. She likes to play a...

$ ml ocr azcv ~/.mlhub/azcv/cache/images/mycat.png | head -2 | sed 's/,/\t/'
51.0 43.0 668.0 51.0 667.0 85.0 51.0 77.0	My cats name is freckles. She like's to cl...
37.0 97.0 691.0 104.0 690.0 134.0 37.0 128.0	high. She is 2 years old. She likes to pla...
```

If you do not care for the bounding boxes that is output by default
from the **ocr** command then simply remove them using
`r Command(cut)`:

```{bash eval=FALSE}
$ ml ocr azcv ~/.mlhub/azcv/cache/images/mycat.png | head -2 | cut -d, -f2-
My cats name is freckles. She like's to climb up
high. She is 2 years old. She likes to play a lot of games.
```

We can process every **jpg** image file in a directory where we may
have several hundred files. We will save the text output into a
**txt** file. The following pipeline utilises a for loop, an `r
Command(ml)` model, and the `r Command(cut)` command:

```{bash eval=FALSE}
$ for f in images/*.jpg; 
  do 
    echo "=====> $f"; 
    ml ocr azcv $f | 
    cut -d, -f2- > $(dirname $f)/$(basename $f .jpg).txt; 
  done
```

Change the two instances of **jpg** to **png** to
process **png** image files, and similarly for
**pdf** files.

Here we transcribe spoken English into text and then translate that
text into Persian (Farsi) using *azspeech2txt* and
*aztranslate*:

```{bash eval=FALSE}
$ ml transcribe azspeech2txt friend.wav | ml translate aztranslate --to=fa
en,1.0,fa,...
,این یک آزمایش است تا ببینید که چگونه همه چیز به خوبی ضبط.
```

A compelling example of a pipeline is to transcribe our English
utterances, translate to French and then synthesise into a female
French voice using a combination of *azspeech* and
*aztranslate*. Here it is:

```{bash eval=FALSE}
$ ml transcribe azspeech | 
  ml translate aztranslate --to=fr | 
  cut -d',' -f4- | 
  ml synthesize azspeech --voice=fr-FR-HortenseRUS
```

<!---------------------------------------------------------------------->
## Adding Bounding Boxes to a Photo
<!---------------------------------------------------------------------->

`r CiteDate(20210317)` Many of the computer vision models will
identify bounding boxes for objects within a photo. For example, the
**faces** command of the [azcv](https://mlhub.au/survivor/azcv.html)
package returns as the first field the bounding box of any faces found
in a photo, one face per line.

The bounding boxes can be added to the photo with the following
script. 

We first download the image from the Internet saving it as the file
`faces.jpg` using `r Command(wget)`. The
[azcv](https://mlhub.au/survivor/azcv.html) model is then called upon
to identify the **faces**, saving the output to a text file
`faces_bb.txt`, contianing the bounding boxes. This text file is con`r
Command(cat)`enated to the `r Command(cut)` command to extract the
first field where fields are denoted by a comma. This field is the
bounding box of each face. Using `r Command(xargs)` and `r
Command(awk)` a command is constructed using `r Command(convert)` from
`r Package(imagemagick)` to draw the blue rectangles of width 3 pixels
for each of the identified faces, saving the resulting image as
`faces_tmp.png`.

```bash
wget https://bit.ly/38GgwPP -O faces.jpg

ml faces azcv faces.jpg | tee faces_bb.txt

cat faces_bb.txt |
  cut -d, -f1 |
  xargs printf '-draw "rectangle %s,%s %s,%s" ' |
  awk '{print "faces.jpg -fill none -stroke blue -strokewidth 3 " $0 "faces_tmp.png"}' |
  xargs convert
```

```{r echo=FALSE, cache=TRUE, fig.align="left", out.width="50%"}
include_graphics_url("https://raw.githubusercontent.com/gjwgit/azcv/master/docs/faces_bb.png")
```

<!---------------------------------------------------------------------->
## azcv faces pipeline
<!---------------------------------------------------------------------->

MERGE WITH ABOVE

`r CiteDate(20210316)` A relatively simple pipeline can add the
bounding boxes for the identified faces to the image.

```{r echo=FALSE, cache=TRUE, fig.align="left", out.width="60%"}
include_graphics_url("https://raw.githubusercontent.com/gjwgit/azcv/master/docs/gala_bb_map.png")
```

We begin by downloading the image locally using `r Command(wget)`:

```bash
$ wget https://g3n1u5.com/mlhub/gala.png -O img.png
```

Now we analyse the image to get the bounding boxes and save it to file
using `r Command(tee)`:

```bash
$ ml faces azcv img.png | tee img_bb.txt
560 152 718 310,Male,53
185 286 326 427,Female,28
```

`r CiteDate("TODO")`

```
$ cat img_bb.txt | cut -d',' -f1 
560 152 718 310
185 286 326 427

$ cat img_bb.txt | cut -d',' -f1  | 
  xargs printf '-draw "rectangle %s,%s %s,%s" '
-draw "rectangle 560,152 718,310" -draw "rectangle 185,286 326,427"

$ cat img_bb.txt | 
  cut -d',' -f1  | 
  xargs printf '-draw "rectangle %s,%s %s,%s" ' | 
  awk '{print "img.png -fill none -stroke red -strokewidth 5 " $0 "img_bb.png"}'
img.png -fill none -stroke red -strokewidth 5 -draw "rectangle 560,152 718,310" -draw "rectangle 185,286 326,427" img_bb.png

$ cat img_bb.txt |
  cut -d',' -f1  | 
  xargs printf '-draw "rectangle %s,%s %s,%s" ' | 
  awk '{print "img.png -fill none -stroke red -strokewidth 5 " $0 "img_bb.png"}' |
  xargs convert
$ display img_bb.png
```

