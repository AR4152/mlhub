# Azure Speech {#azspeech}

`r CiteDate(20200806)` The [MLHub](https://mlhub.ai) package
[azspeech](https://github.com/gjwgit/azspeech) utilises pre-built
speech models provided through Azure's Cognitive Services. This
service can, for example, take an audio signal and transcribe it to
return the text. It also supports speech synthesis, taking text and
synthesising a voice to read the text with multiple voices and
languages available.

A free Azure subscription allowing up to 5,000 free transactions per
month is available from <https://azure.microsoft.com/free/>. After
subscribing visit the [Azure Portal](https://ms.portal.azure.com) to
create a Speech resource. Once reated you can access the web API
subscription key and endpoint from the portal. These will be prompted
for when running a command (e.g., **demo**), and then saved to file to
reduce the need for repeated authentication requests.

To install, configure, and demonstrate the package:

```bash
ml install azspeech
ml configure azspeech
ml readme azspeech
ml commands azspeech
ml demo azspeech
ml gui azspeech
```

In addition to the `r ml_command(azspeech, demo)` command the package
supports `r ml_command(azspeech,synthesize)`, and 
`r ml_command(azspeech,transcribe)` 

The source code for this MLHub package is available from github:
<https://github.com/gjwgit/azspeech>.

*Azure-based models, unlike the MLHub models in general, use **closed
source services** which have no guarantee of ongoing availability and
do not come with the freedom to modify and share. This cloud based
service also sends your text (for synthesis) and audio (for
transcription) to the Axure for analysis.*

<!---------------------------------------------------------------------->
## azspeech quick start
<!---------------------------------------------------------------------->

```console
$ wget https://github.com/realpython/python-speech-recognition/raw/master/audio_files/harvard.wav

$ ml synthesize azspeech Welcome my friend, welcome to the machine.
$ ml synthesize azspeech --file=ai.txt

$ ml transcribe azspeech
$ ml transcribe azspeech --file=harvard.wav
```

<!---------------------------------------------------------------------->
## azspeech demo
<!---------------------------------------------------------------------->

```console
$ ml demo azspeech 

===============
Speech Services
===============

Welcome to a demo of the pre-built models for Speech provided through
Azure's Cognitive Services. The Speech cloud service  supports speech
to text and text to speech capabilities.

The following file has been found and is assumed to contain an Azure 
subscription key and location for Speech. We will load 
the file and use this information.

    /home/gjw/.mlhub/azspeech/private.txt

Press Enter and then say something: 

> Recognized: Welcome to a demo of the prebuilt models speech provided 
> through Azure as cognitive services. The speech cloud service provides
> speech to text and text to speech capabilities.

Press Enter to continue: 

Now type text to be spoken. When Enter is pressed you will hear the result.

> Welcome to a demo of the prebuilt models for speech.
```

The first paragraph from the screen was read and the Azure Speech to
Text service was mostly accurate in its transcription. For synthesis
the same text was used and could be heard through the system speakers.

<!---------------------------------------------------------------------->
## azspeech synthesize
<!---------------------------------------------------------------------->

The **synthesize** command will generate spoken word audio, spoken by
a human sounding voice, from supplied text, and will play the audio on
the system's default audio output. With `-o` or `--output` a *wav*
file can be specified as the output rather than having the audio
played through the speakers.

```console
$ ml synthesize azspeech [sentence]
     -f <file>     --file=<file> 	    Text to be spoken.
     -l <lang>     --lang=<lang>
     -o <file.wav> --output=<file.wav>  Save synthesized audio to file.
     -v <voice>    --voice=<voice>
```

Examples

```console
$ ml synthesize azspeech Welcome my son, welcome to the machine.

$ ml synthesize azspeech --lang=fr-FR It's alright, we know where you've been.

$ ml synthesize azspeech --voice=en-AU-NatashaNeural You brought a guitar to punish your ma.

$ echo "It's alright, we told you what to dream" | ml synthesize azspeech

$ ml synthesize azspeech --file=short.txt

$ ml synthesize azspeech --lang=de-DE --file=short.txt

$ ml synthesize azspeech --voice=fr-FR-DeniseNeural --file=short.txt
```

The supported languages and their locale codes (BCP-47) are listed at [Azure
Docs](https://docs.microsoft.com/en-gb/azure/cognitive-services/speech-service/language-support).

<!---------------------------------------------------------------------->
## azspeech transcribe
<!---------------------------------------------------------------------->

The **transcribe** command will listen for an utterance from the
computer microphone for up to 15 seconds and then transcribe it
(convert to text) to standard output. The command can also be used to
transcribe speech from an audio file (*wav* only).

```console
$ ml transcribe azspeech 
     -f <file.wav>   --file=<file.wav>
```

By default the audio input is from the computer's microphone:

```console
$ ml transcribe azspeech
The machine learning hub is useful for demonstrating capability of 
models as well as providing command line tools.
```

We can pipe the output to other tools, so as to analyse the sentiment
of the spoken word. In the first instance you might say *happy days*
and in the second say *sad days*.

```console
$ ml transcribe azspeech | ml sentiment aztext
0.96

$ ml transcribe azspeech | ml sentiment aztext
0.07
```

The **transcribe** command can take an audio (wav) file and transcribe
it to standard output. For large audio files this will take extra
time. Currently only wav files are supported through the command line
(though the service also supports mp3, ogg, and flac).

```console
$ wget https://github.com/realpython/python-speech-recognition/raw/master/audio_files/harvard.wav
$ ml transcribe azspeech --file=harvard.wav
The stale smell of old beer lingers it takes heat to bring out the odor.
A cold dip restore's health and Zest, a salt pickle taste fine with
Ham tacos, Al Pastore are my favorite a zestful food is the hot cross bun.
```

**Spoken English Input to Spoken French Output**

```console
$ ml transcribe azspeech |
  ml translate aztranslate --to=fr |
  cut -d',' -f4- |
  ml synthesize azspeech --voice=fr-FR-HortenseRUS
```

<!---------------------------------------------------------------------->
## azspeech resources
<!---------------------------------------------------------------------->

* [MLHub](https://mlhub.ai)

* [Speech Services
  Documentation](https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/)

* [Supported
  Languages](https://docs.microsoft.com/en-gb/azure/cognitive-services/speech-service/language-support)

* [Python code for Speech Recognizer:
  Speech2Text](https://github.com/Azure-Samples/cognitive-services-speech-sdk/blob/master/quickstart/python/from-microphone)
  
* [Python code for Speech Synthesizer:
  Text2Speech](https://github.com/Azure-Samples/cognitive-services-speech-sdk/blob/master/quickstart/python/text-to-speech)
